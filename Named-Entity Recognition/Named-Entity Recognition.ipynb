{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/randomspace/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/randomspace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word    Tag\n",
       "0  Sentence: 1      Thousands      O\n",
       "1  Sentence: 1             of      O\n",
       "2  Sentence: 1  demonstrators      O\n",
       "3  Sentence: 1           have      O\n",
       "4  Sentence: 1        marched      O\n",
       "5  Sentence: 1        through      O\n",
       "6  Sentence: 1         London  B-geo\n",
       "7  Sentence: 1             to      O\n",
       "8  Sentence: 1        protest      O\n",
       "9  Sentence: 1            the      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.drop(['POS'], axis =1)\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Sentence #               Word    Tag\n",
      "264    Sentence: 12  Non-Proliferation  I-art\n",
      "3811  Sentence: 171                V-6  I-art\n",
      "4016  Sentence: 183             Simple  I-art\n",
      "4017  Sentence: 183               Life  I-art\n",
      "4142  Sentence: 188            Morning  I-art\n",
      "4143  Sentence: 188            America  I-art\n",
      "5248  Sentence: 236             Mirror  I-art\n",
      "5923  Sentence: 270                 De  I-art\n",
      "5924  Sentence: 270             Gaulle  I-art\n",
      "5935  Sentence: 270      International  I-art\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[data['Tag'] == 'I-art'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('Iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35179\n",
      "['channeled', 'freed', 'Oath', 'non-peaceful', 'Veligonda', 'Freezing', 'halls', 'whistleblower', 'changing', 'THAKSIN', 'Hirayama', 'Kani', 'hunters', 'multiple-use', 'Atlantic', 'Mahendra', 'Fame', 'embattled', 'wanderings', 'Benshoofin', 'alarmist', 'convoy', 'Nandurbar', 'unthinkable', 'selected', 'rebelled', 'involved', 'Alexandre', 'Srinagar', 'Hashimzai', '60,000-seat', 'separates', 'Trustco', 'Khesar', 'brokerage', 'tightening', 'condemned', 'barge', 'al-Attiya', 'medieval', 'Simeus', 'preparing', 'Bishops', 'breaching', 'convened', 'Krajina', '256-AD', 'defamation', 'never', 'meters-deep', 'Jones', 'Namgyel', 'respect', 'Current', 'uranium-enriching', 'dwellers', 'prodemocracy', 'reauthorize', '40-million', 'Agrio', 'Nadeem', 'joke', 'outpost', 'Morse', 'DIA', 'scrambling', 'Muttawakil', 'Pedersen', 'opined', 'Tarawa', 'Elsa', 'Ambrosio', 'conga', 'Sonia', 'runway', 'Baya', 'fiercer', 'abrupt', 'Kyodo', 'threat', 'painting', 'Ciamis', 'Gene', 'legacy', 'pie', '1963', 'nominally', 'boss', 'career', 'Katyusha', 'musician', 'detractors', 'keyboard', 'averaged', 'slippery', 'del', 'incorrect', 'tablets', 'Gandolfo', 'inflicted', 'euro', 'divisive', 'buys', 'totally', 'snapping', 'Association', 'drifted', 'diligently', 'swans', '1755', 'Salehi', 'non-Jewish', 'inhabited', 'near-continuous', 'Kartli-Iberia', 'three-fourths', '47', 'nation-states', 'Ivanovo', 'Tshabalala', 'Pinochet', 'Iskandariya', 'de-furred', 'backlog', '10,00,000', 'African-American', 'Tennessee', 'buy-back', 'nationalism', 'mine-clearing', 'lesson', 'dilemma', 'refineries', 'Moldavia', 'POP', 'injuring', 'flaring', 'Biscaglia', 'inferior', 'Bakiyev', 'anticipations', 'virtually', 'wrongly', 'decommission', 'Mansoor', 'Mujahed', 'Coalition', 'Goodall', 'Celebrity', 'al-Sabah', 'Simply', 'Inspector', 'Medina', 'regulators', 'arid', '#', 'picnics', 'erupt', 'Ulemas', 'hard-core', 'encounter', 'Hector', 'rice-growing', '720', 'volcanic', 'enlist', 'contaminant', 'Davenport', 'holds', 'Naing', 'largest-ever', 'check-up', 'Competitive', 'GE', 'skullcaps', 'Llodra', 'al-Qassam', 'join', 'base-closing', 'Sgt.', 'wide-scale', 'See', 'safe', 'ground', 'Jessica', 'neighborhoods', 'Feyzabad', 'Domino', 'none', '12-hour', 'temperature', 'postwar', 'SICS', 'Veloso', 'Advocates', 'Alarcon', 'Facing', 'retreat', 'dwelling', 'opera']\n"
     ]
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "print(n_words)\n",
    "print(words[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['B-art', 'I-geo', 'O', 'B-tim', 'B-nat', 'I-tim', 'I-org', 'B-eve', 'I-per', 'I-eve', 'B-org', 'I-gpe', 'I-art', 'B-per', 'I-nat', 'B-geo', 'B-gpe']\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(n_tags)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\",value=word2idx[\"ENDPAD\"])\n",
    "\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31271  2221  4902 16388 26684 32954  1069 22647 22134  4260 28926 27701\n",
      " 28536 27748 17782  4260 27116  2221  6123 10894 21132 28979 15273 19769\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178 35178\n",
      " 35178 35178]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n"
     ]
    }
   ],
   "source": [
    "print(words[1069])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38367\n",
      "38367\n"
     ]
    }
   ],
   "source": [
    "print(len(X_tr))\n",
    "print(len(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9592\n",
      "9592\n"
     ]
    }
   ],
   "source": [
    "print(len(X_te))\n",
    "print(len(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 13:37:33.755209 140458385430336 deprecation_wrapper.py:119] From /home/randomspace/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0830 13:37:33.757559 140458385430336 deprecation_wrapper.py:119] From /home/randomspace/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0830 13:37:33.763205 140458385430336 deprecation_wrapper.py:119] From /home/randomspace/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0830 13:37:33.772974 140458385430336 deprecation_wrapper.py:119] From /home/randomspace/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0830 13:37:33.780175 140458385430336 deprecation.py:506] From /home/randomspace/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0830 13:37:34.941345 140458385430336 deprecation_wrapper.py:119] From /home/randomspace/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0830 13:37:34.955506 140458385430336 deprecation_wrapper.py:119] From /home/randomspace/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=max_len, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=400, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 13:37:38.525434 140458385430336 deprecation.py:323] From /home/randomspace/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30693 samples, validate on 7674 samples\n",
      "Epoch 1/3\n",
      "30693/30693 [==============================] - 140s 5ms/step - loss: 0.1654 - acc: 0.9598 - val_loss: 0.0653 - val_acc: 0.9813\n",
      "Epoch 2/3\n",
      "30693/30693 [==============================] - 138s 4ms/step - loss: 0.0504 - acc: 0.9849 - val_loss: 0.0520 - val_acc: 0.9845\n",
      "Epoch 3/3\n",
      "30693/30693 [==============================] - 140s 5ms/step - loss: 0.0369 - acc: 0.9887 - val_loss: 0.0485 - val_acc: 0.9856\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(X_tr,\\\n",
    "                    np.array(y_tr),\\\n",
    "                    batch_size = 32,\\\n",
    "                    epochs = 3,\\\n",
    "                    validation_split = 0.2,\\\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 50)            1758950   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 800)           1443200   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 17)            13617     \n",
      "=================================================================\n",
      "Total params: 3,215,767\n",
      "Trainable params: 3,215,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9592/9592 [==============================] - 18s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_pred = model.predict(X_te, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randomspace/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        71\n",
      "       B-eve       0.82      0.18      0.29        78\n",
      "       B-geo       0.86      0.88      0.87      7527\n",
      "       B-gpe       0.97      0.93      0.95      3126\n",
      "       B-nat       0.00      0.00      0.00        39\n",
      "       B-org       0.78      0.71      0.74      4011\n",
      "       B-per       0.85      0.80      0.82      3497\n",
      "       B-tim       0.94      0.86      0.90      4085\n",
      "       I-art       0.00      0.00      0.00        49\n",
      "       I-eve       0.00      0.00      0.00        69\n",
      "       I-geo       0.80      0.80      0.80      1485\n",
      "       I-gpe       1.00      0.58      0.74        36\n",
      "       I-nat       0.00      0.00      0.00        12\n",
      "       I-org       0.76      0.78      0.77      3295\n",
      "       I-per       0.87      0.82      0.85      3575\n",
      "       I-tim       0.83      0.74      0.78      1311\n",
      "           O       0.99      1.00      1.00    447334\n",
      "\n",
      "    accuracy                           0.99    479600\n",
      "   macro avg       0.62      0.53      0.56    479600\n",
      "weighted avg       0.98      0.99      0.99    479600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p, axis = -1)\n",
    "            out.append(idx2tag[p_i])\n",
    "    return out\n",
    "\n",
    "pred_labels = pred2label(test_pred)\n",
    "true_labels = pred2label(y_te)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test user's text\n",
    "test_text = \"The 58-year-old shared dozens of photos to Facebook of an early August jaunt to Miami with Christopher Jr. for the National Association of Black Journalists conference, followed by a trip she took with Erin to Italy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens = word_tokenize(test_text)\n",
    "X_test = []\n",
    "for token in list_tokens:\n",
    "    if token not in word2idx:\n",
    "        word2idx[token] = len(word2idx)\n",
    "        words.append(token)\n",
    "    X_test.append(word2idx[token])\n",
    "\n",
    "X_test = pad_sequences(maxlen=max_len, sequences=[X_test], padding=\"post\",value=word2idx[\"ENDPAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           (Pred)\n",
      "The           : O\n",
      "58-year-old   : O\n",
      "shared        : O\n",
      "dozens        : O\n",
      "of            : O\n",
      "photos        : O\n",
      "to            : O\n",
      "Facebook      : B-org\n",
      "of            : O\n",
      "an            : O\n",
      "early         : O\n",
      "August        : B-tim\n",
      "jaunt         : I-tim\n",
      "to            : O\n",
      "Miami         : B-geo\n",
      "with          : O\n",
      "Christopher   : B-per\n",
      "Jr.           : I-per\n",
      "for           : O\n",
      "the           : O\n",
      "National      : B-org\n",
      "Association   : I-org\n",
      "of            : I-org\n",
      "Black         : I-org\n",
      "Journalists   : I-org\n",
      "conference    : O\n",
      ",             : O\n",
      "followed      : O\n",
      "by            : O\n",
      "a             : O\n",
      "trip          : O\n",
      "she           : O\n",
      "took          : O\n",
      "with          : O\n",
      "Erin          : B-per\n",
      "to            : O\n",
      "Italy         : B-geo\n",
      ".             : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n",
      "ENDPAD        : O\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test)\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:14} ({:4})\".format(\"Word\", \"Pred\"))\n",
    "for w,pred in zip(X_test[0],p[0]):\n",
    "    print(\"{:14}: {}\".format(words[w],tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
